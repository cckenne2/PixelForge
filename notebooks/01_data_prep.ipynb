{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b897c9a",
   "metadata": {},
   "source": [
    "### Imports & Paths (no data is downloaded here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# GPU setup and verification\n",
    "def check_gpu_setup():\n",
    "    \"\"\"Check CUDA availability and GPU memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        current_gpu = torch.cuda.current_device()\n",
    "        gpu_name = torch.cuda.get_device_name(current_gpu)\n",
    "        gpu_memory = torch.cuda.get_device_properties(current_gpu).total_memory / 1024**3\n",
    "        print(f\"✓ CUDA available with {gpu_count} GPU(s)\")\n",
    "        print(f\"✓ Using GPU {current_gpu}: {gpu_name}\")\n",
    "        print(f\"✓ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"⚠ CUDA not available, falling back to CPU\")\n",
    "        return False\n",
    "\n",
    "# Check GPU setup\n",
    "CUDA_AVAILABLE = check_gpu_setup()\n",
    "\n",
    "# Detect the operating system and set the data root path accordingly\n",
    "if Path(\"/mnt/c\").exists():  # Windows Subsystem for Linux (WSL)\n",
    "    DATA_ROOT = Path(\"../data\")  # WSL path\n",
    "elif Path(\"D:/data\").exists():  # Native Windows\n",
    "    DATA_ROOT = Path(\"..\\\\data\")  # Windows path\n",
    "else:  # Assume Linux\n",
    "    DATA_ROOT = Path(\"../data\")  # Linux (debian-based) path\n",
    "try:\n",
    "    assert DATA_ROOT.exists()\n",
    "except AssertionError:\n",
    "    raise FileNotFoundError(f\"Data root directory {DATA_ROOT} does not exist.\")\n",
    "HR_VALID = DATA_ROOT / \"HR\" / \"valid\"\n",
    "HR_TRAIN = DATA_ROOT / \"HR\" / \"train\"\n",
    "IMAGE_TYPES = [\".png\", \".jpg\", \".jpeg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_dir(scale: int, split: str) -> Path:\n",
    "    return DATA_ROOT / f\"LR_bicubic/X{scale}\" / split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca92e1",
   "metadata": {},
   "source": [
    "Sanity check: do we see the HR images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea36e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(p: Path):\n",
    "    return len([x for x in p.glob(\"*\") if x.suffix.lower() in IMAGE_TYPES])\n",
    "\n",
    "print(\"HR train dir:\", HR_TRAIN.resolve())\n",
    "print(\"HR valid dir:\", HR_VALID.resolve())\n",
    "print(\"HR train count:\", count_images(HR_TRAIN))\n",
    "print(\"HR valid count:\", count_images(HR_VALID))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca0876b",
   "metadata": {},
   "source": [
    "### Helper to Generate a LR Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a61f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    if not p.exists():\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def hr_to_lr(hr_path: Path, out_dir: Path, scale: int):\n",
    "    out_path = out_dir / hr_path.name\n",
    "    if out_path.exists():\n",
    "        return # Skip existing files\n",
    "    \n",
    "    # Use GPU if available, otherwise CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load image and convert to tensor\n",
    "    img = Image.open(hr_path).convert(\"RGB\")\n",
    "    w, h = img.size\n",
    "    \n",
    "    # Crop to make dimensions divisible by scale\n",
    "    w2, h2 = w - (w % scale), h - (h % scale)\n",
    "    if (w2, h2) != (w, h):\n",
    "        img = img.crop((0, 0, w2, h2))\n",
    "    \n",
    "    # Convert PIL to tensor and move to GPU\n",
    "    img_tensor = TF.to_tensor(img).unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n",
    "    \n",
    "    # Calculate new dimensions\n",
    "    new_h, new_w = h2 // scale, w2 // scale\n",
    "    \n",
    "    # Resize using PyTorch's bicubic interpolation on GPU\n",
    "    lr_tensor = F.interpolate(\n",
    "        img_tensor, \n",
    "        size=(new_h, new_w), \n",
    "        mode='bicubic', \n",
    "        align_corners=False,\n",
    "        antialias=True\n",
    "    )\n",
    "    \n",
    "    # Convert back to PIL for saving\n",
    "    lr_tensor = lr_tensor.squeeze(0).cpu()  # Remove batch dim and move to CPU\n",
    "    lr_img = TF.to_pil_image(lr_tensor)\n",
    "    \n",
    "    # Save the result\n",
    "    ensure_dir(out_dir)\n",
    "    lr_img.save(out_path, quality=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890dc8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def hr_to_lr_batch(hr_paths: list, out_dir: Path, scale: int, batch_size: int = 8):\n",
    "    \"\"\"Process multiple images in batches for better GPU utilization, grouping by shape\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    # Group images by their cropped size\n",
    "    shape_groups = defaultdict(list)\n",
    "    for hr_path in hr_paths:\n",
    "        out_path = out_dir / hr_path.name\n",
    "        if out_path.exists():\n",
    "            continue  # Skip existing files\n",
    "\n",
    "        img = Image.open(hr_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "        w2, h2 = w - (w % scale), h - (h % scale)\n",
    "        shape_groups[(w2, h2)].append(hr_path)\n",
    "\n",
    "    # Process each shape group separately\n",
    "    for (w2, h2), paths in shape_groups.items():\n",
    "        for i in range(0, len(paths), batch_size):\n",
    "            batch_paths = paths[i:i+batch_size]\n",
    "            batch_tensors = []\n",
    "            valid_paths = []\n",
    "\n",
    "            for hr_path in batch_paths:\n",
    "                img = Image.open(hr_path).convert(\"RGB\")\n",
    "                img = img.crop((0, 0, w2, h2))\n",
    "                img_tensor = TF.to_tensor(img)\n",
    "                batch_tensors.append(img_tensor)\n",
    "                valid_paths.append(hr_path)\n",
    "\n",
    "            if not batch_tensors:\n",
    "                continue\n",
    "\n",
    "            batch_tensor = torch.stack(batch_tensors).to(device)\n",
    "            with torch.no_grad():\n",
    "                new_h, new_w = h2 // scale, w2 // scale\n",
    "                lr_batch = F.interpolate(\n",
    "                    batch_tensor,\n",
    "                    size=(new_h, new_w),\n",
    "                    mode='bicubic',\n",
    "                    align_corners=False,\n",
    "                    antialias=True\n",
    "                )\n",
    "\n",
    "            lr_batch_cpu = lr_batch.cpu()\n",
    "            for j, hr_path in enumerate(valid_paths):\n",
    "                out_path = out_dir / hr_path.name\n",
    "                lr_tensor = lr_batch_cpu[j]\n",
    "                lr_img = TF.to_pil_image(lr_tensor)\n",
    "                lr_img.save(out_path, quality=95)\n",
    "\n",
    "            del batch_tensor, lr_batch\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fadd9d1",
   "metadata": {},
   "source": [
    "Tiny smoke test (valid split, X4, first 5 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 4\n",
    "SPLIT = \"valid\"\n",
    "OUT_DIR = lr_dir(SCALE, SPLIT)\n",
    "\n",
    "valid_imgs = sorted([p for p in HR_VALID.glob(\"*\") if p.suffix.lower() in IMAGE_TYPES])\n",
    "subset = valid_imgs[:5] # Take the first 5 images\n",
    "\n",
    "for p in tqdm(subset, desc=f\"Generating X{SCALE} ({SPLIT})\"):\n",
    "    hr_to_lr(p, OUT_DIR, SCALE)\n",
    "\n",
    "print(\"Wrote to:\", OUT_DIR.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8385be36",
   "metadata": {},
   "source": [
    "### Small Batch Function (Choose Split + Scale + Limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flexible wrapper to perform low-level image operations with GPU acceleration\n",
    "def generate_lr_split(scale: int, split : str, limit: int | None = None, verbose: bool = True, use_batch: bool = True, batch_size: int = 16):\n",
    "    hr_dir = HR_TRAIN if split == \"train\" else HR_VALID\n",
    "    out_dir = lr_dir(scale, split)\n",
    "    imgs = sorted([p for p in hr_dir.glob(\"*\") if p.suffix.lower() in IMAGE_TYPES])\n",
    "    if limit is not None:\n",
    "        imgs = imgs[:limit]\n",
    "    \n",
    "    if use_batch and torch.cuda.is_available():\n",
    "        # Use batch processing for better GPU utilization\n",
    "        hr_to_lr_batch(imgs, out_dir, scale, batch_size)\n",
    "        if verbose:\n",
    "            print(f\"Done (GPU batch): {out_dir.resolve()} ({count_images(out_dir)} images)\")\n",
    "    else:\n",
    "        # Fall back to sequential processing\n",
    "        for p in tqdm(imgs, desc=f\"X{scale} {split} ({len(imgs)} imgs)\"):\n",
    "            hr_to_lr(p, out_dir, scale)\n",
    "        if verbose:\n",
    "            print(f\"Done: {out_dir.resolve()} ({count_images(out_dir)} images)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c624955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_gpu_settings():\n",
    "    \"\"\"Optimize GPU settings for better performance\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Enable mixed precision for faster processing\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"✓ GPU optimizations enabled\")\n",
    "        print(f\"✓ Current GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        print(f\"✓ Current GPU memory cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    else:\n",
    "        print(\"⚠ No CUDA GPU available for optimization\")\n",
    "\n",
    "def get_optimal_batch_size(scale: int) -> int:\n",
    "    \"\"\"Determine optimal batch size based on GPU memory and scale factor\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return 1\n",
    "    \n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    \n",
    "    # Estimate memory usage per image (rough approximation)\n",
    "    # Higher scales need less memory per LR image but more for processing\n",
    "    if scale == 2:\n",
    "        return min(32, max(8, int(gpu_memory_gb * 2)))\n",
    "    elif scale == 4:\n",
    "        return min(24, max(6, int(gpu_memory_gb * 1.5)))\n",
    "    elif scale == 8:\n",
    "        return min(16, max(4, int(gpu_memory_gb)))\n",
    "    else:\n",
    "        return 8\n",
    "\n",
    "# Initialize GPU optimizations\n",
    "optimize_gpu_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52f36b1",
   "metadata": {},
   "source": [
    "Try slightly larger test to confirm functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c4ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_lr_split(scale=2, split=\"train\", limit=20, use_batch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726c6ee",
   "metadata": {},
   "source": [
    "### Generate Full LR Sets (x2, x4, x8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde67931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run generation for all splits and scales with GPU optimization\n",
    "SCALES = [2, 4, 8]\n",
    "\n",
    "def generate_all_scales_for_split(split: str):\n",
    "    assert split in {\"train\", \"valid\"}\n",
    "    for s in SCALES:\n",
    "        # Use optimal batch size for each scale\n",
    "        optimal_batch_size = get_optimal_batch_size(s) if torch.cuda.is_available() else 1\n",
    "        print(f\"Processing scale X{s} with batch size {optimal_batch_size}\")\n",
    "        \n",
    "        # Use the modified function with optimal batch size\n",
    "        hr_dir = HR_TRAIN if split == \"train\" else HR_VALID\n",
    "        out_dir = lr_dir(s, split)\n",
    "        imgs = sorted([p for p in hr_dir.glob(\"*\") if p.suffix.lower() in IMAGE_TYPES])\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            hr_to_lr_batch(imgs, out_dir, s, optimal_batch_size)\n",
    "            print(f\"Done (GPU batch): {out_dir.resolve()} ({count_images(out_dir)} images)\")\n",
    "        else:\n",
    "            for p in tqdm(imgs, desc=f\"X{s} {split} ({len(imgs)} imgs)\"):\n",
    "                hr_to_lr(p, out_dir, s)\n",
    "            print(f\"Done: {out_dir.resolve()} ({count_images(out_dir)} images)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_gpu_memory():\n",
    "    \"\"\"Monitor and display current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f}GB | Reserved: {reserved:.2f}GB | Total: {total:.1f}GB\")\n",
    "        print(f\"Memory utilization: {(allocated/total)*100:.1f}%\")\n",
    "    else:\n",
    "        print(\"No CUDA GPU available for monitoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0112f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU memory before processing\n",
    "print(\"=== Initial GPU Memory Status ===\")\n",
    "monitor_gpu_memory()\n",
    "\n",
    "# Perform splits on all scales\n",
    "print(\"\\n== VALID ==\")\n",
    "generate_all_scales_for_split(\"valid\")\n",
    "print(\"\\n=== GPU Memory After Valid ===\")\n",
    "monitor_gpu_memory()\n",
    "\n",
    "print(\"\\n== TRAIN ==\")\n",
    "generate_all_scales_for_split(\"train\")\n",
    "print(\"\\n=== Final GPU Memory Status ===\")\n",
    "monitor_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b1bcb3",
   "metadata": {},
   "source": [
    "Integrity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055675f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking integrity and validity of generated images\n",
    "from PIL import Image\n",
    "\n",
    "def image_list(p):\n",
    "    return sorted([x for x in p.glob(\"*\") if x.suffix.lower() in IMAGE_TYPES])\n",
    "\n",
    "def check_image_integrity(image_path: Path):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        img.verify()  # Verify the image is not corrupted\n",
    "        return True\n",
    "    except (IOError, SyntaxError) as e:\n",
    "        print(f\"Corrupted image {image_path}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8335765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the integrity of the dataset splits\n",
    "def check_split(split=\"valid\", max_check=None):\n",
    "    hr_dir  = HR_TRAIN if split == \"train\" else HR_VALID\n",
    "    hr_imgs = image_list(hr_dir)\n",
    "    if max_check is not None:\n",
    "        hr_imgs = hr_imgs[:max_check]\n",
    "    \n",
    "    problems = []\n",
    "\n",
    "    for hr_path in hr_imgs:\n",
    "        hr = Image.open(hr_path).convert(\"RGB\")\n",
    "        w, h = hr.size\n",
    "        for s in SCALES:\n",
    "            lr_path = lr_dir(s, split) / hr_path.name\n",
    "            if not lr_path.exists():\n",
    "                problems.append((hr_path.name, f\"missing LR X{s}\", str(lr_path)))\n",
    "                continue\n",
    "            lr = Image.open(lr_path).convert(\"RGB\")\n",
    "            ew, eh = w // s, h // s  # Expected size\n",
    "            if lr.size != (ew, eh):\n",
    "                problems.append((hr_path.name, f\"size mismatch X{s}\", f\"got={lr.size}, expected=({ew}, {eh})\"))\n",
    "    return problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02dfb5d",
   "metadata": {},
   "source": [
    " Test valid images first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0721364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_valid = check_split(split=\"valid\")\n",
    "if problems_valid:\n",
    "    print(f\"[FAIL] Found {len(problems_valid)} problems (showing first 10):\")\n",
    "    for row in problems_valid[:10]:\n",
    "        print(\" -\", row)\n",
    "else:\n",
    "    print(\"[OK] VALID split looks consistent for all images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cde6b8",
   "metadata": {},
   "source": [
    "Same check for train (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b3db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_train = check_split(split=\"train\")\n",
    "if problems_train:\n",
    "    print(f\"[FAIL] Found {len(problems_train)} problems (showing first 10):\")\n",
    "    for row in problems_train[:10]:\n",
    "        print(\" -\", row)\n",
    "else:\n",
    "    print(\"[OK] TRAIN split looks consistent for all images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48177961",
   "metadata": {},
   "source": [
    "(Optional) Visual spot-check of one triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1fcf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython.display import display\n",
    "\n",
    "split = \"valid\"\n",
    "hr_dir = HR_VALID if split == \"valid\" else HR_TRAIN\n",
    "hr_imgs = image_list(hr_dir)\n",
    "random.seed(0)\n",
    "hr_path = random.choice(hr_imgs)\n",
    "\n",
    "im_hr = Image.open(hr_path).convert(\"RGB\")\n",
    "im_x2 = Image.open(lr_dir(2, split) / hr_path.name).convert(\"RGB\")\n",
    "im_x4 = Image.open(lr_dir(4, split) / hr_path.name).convert(\"RGB\")\n",
    "im_x8 = Image.open(lr_dir(8, split) / hr_path.name).convert(\"RGB\")\n",
    "\n",
    "print(\"Picked:\", hr_path.name)\n",
    "print(\"HR:\", im_hr.size, \" | X2:\", im_x2.size, \" | X4:\", im_x4.size, \" | X8:\", im_x8.size)\n",
    "display(im_hr.resize((im_hr.size[0]//4, im_hr.size[1]//4)))  # shrink for notebook view\n",
    "display(im_x2)\n",
    "display(im_x4)\n",
    "display(im_x8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01740472",
   "metadata": {},
   "source": [
    "### Import PyTorch and Set Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a58732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "def pil_to_tensor(img: Image.Image) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    WHAT: Convert a PIL image (H x W x 3, uint8) to a float tensor in [0,1] shaped (3, H, W).\n",
    "    WHY: PyTorch models expect CHW tensors, not PIL images.\n",
    "    \"\"\"\n",
    "    t = TF.to_tensor(img)  # scales to [0,1], returns float32 CHW\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c3093a",
   "metadata": {},
   "source": [
    "Minimal dataset for aligned LR/HR pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f486b9c",
   "metadata": {},
   "source": [
    "What this does:\n",
    "\n",
    "You choose a split (\"train\" or \"valid\") and a scale (2, 4, or 8).\n",
    "\n",
    "It finds all HR files, then loads the matching LR file with the same filename from LR_bicubic/X{scale}/{split}.\n",
    "\n",
    "It returns a dict with lr, hr, filename, and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd29ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class\n",
    "class DIV2KDataset(Dataset):\n",
    "    \"\"\"\n",
    "    WHAT: Paired dataset that yields {\"lr\": tensor, \"hr\": tensor, \"filename\": str, \"scale\": int}\n",
    "    WHY: Training needs perfectly aligned LR-HR pairs for stable learning.\n",
    "    \"\"\"\n",
    "    # Define data paths\n",
    "    def __init__(self, data_root: Path, split: str=\"train\", scale: int=4, patch_size: int | None = None):\n",
    "        # Validate inputs\n",
    "        assert split in {\"train\", \"valid\"}, \"split must be 'train' or 'valid'\"\n",
    "        assert scale in {2, 4, 8}, \"scale must be one of 2, 4, or 8\"\n",
    "\n",
    "        # Store parameters\n",
    "        self.data_root = Path(data_root)\n",
    "        self.split = split\n",
    "        self.scale = scale\n",
    "        self.patch_size = patch_size # HR patch size\n",
    "\n",
    "        # Directories\n",
    "        self.hr_dir = (self.data_root / \"HR\" / split)\n",
    "        self.lr_dir = (self.data_root / \"LR_bicubic\" / f\"X{scale}\" / split)\n",
    "\n",
    "        # List all HR images and keep only those that have a matching LR\n",
    "        exts = {\".png\", \".jpg\", \".jpeg\"}\n",
    "        hr_paths = sorted([p for p in self.hr_dir.iterdir() if p.suffix.lower() in exts])\n",
    "\n",
    "        # Keep only items with matching LR images\n",
    "        self.items = []\n",
    "        for hrp in hr_paths:\n",
    "            lrp = self.lr_dir / hrp.name\n",
    "            if lrp.exists():\n",
    "                self.items.append((lrp, hrp))\n",
    "            else:\n",
    "                print(f\"Warning: Missing LR image for {hrp.name}, skipping.\")\n",
    "                pass\n",
    "        \n",
    "        if len(self.items) == 0:\n",
    "            raise RuntimeError(f\"No paired items found for split={split}, scale=X{scale}.\\\n",
    "                               \\nCheck folders: {self.lr_dir} and {self.hr_dir}\")\n",
    "\n",
    "    # Dataset length\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    # Aligned random crop helper\n",
    "    def _aligned_random_crop(self, lr_img: Image.Image, hr_img: Image.Image, ps: int):\n",
    "        \"\"\"\n",
    "        Crop an HR patch of size (ps x ps) at a random location,\n",
    "        and the matching LR patch at (ps/scale x ps/scale).\n",
    "        \"\"\"\n",
    "        assert ps % self.scale == 0, f\"patch_size ({ps}) must be divisible by scale ({self.scale})\"\n",
    "        lr_ps = ps // self.scale\n",
    "\n",
    "        # Random top-left in HR space\n",
    "        max_x = hr_img.width  - ps\n",
    "        max_y = hr_img.height - ps\n",
    "        if max_x < 0 or max_y < 0:\n",
    "            raise ValueError(f\"HR image smaller than patch_size={ps}: got {(hr_img.width, hr_img.height)}\")\n",
    "\n",
    "        # Choose coordinates (0 allowed so +1 only if max>0)\n",
    "        x = 0 if max_x == 0 else torch.randint(0, max_x + 1, (1,)).item()\n",
    "        y = 0 if max_y == 0 else torch.randint(0, max_y + 1, (1,)).item()\n",
    "\n",
    "        # HR crop\n",
    "        hr_crop = hr_img.crop((x, y, x + ps, y + ps))\n",
    "\n",
    "        # Corresponding LR coords (scaled down)\n",
    "        lr_x, lr_y = x // self.scale, y // self.scale\n",
    "        lr_crop = lr_img.crop((lr_x, lr_y, lr_x + lr_ps, lr_y + lr_ps))\n",
    "\n",
    "        return lr_crop, hr_crop\n",
    "\n",
    "    # Convert images to tensors\n",
    "    def __getitem__(self, idx: int):\n",
    "        lrp, hrp = self.items[idx]\n",
    "\n",
    "        # Load as PIL\n",
    "        lr_img = Image.open(lrp).convert(\"RGB\")\n",
    "        hr_img = Image.open(hrp).convert(\"RGB\")\n",
    "\n",
    "        # Aligned crop\n",
    "        if self.patch_size is not None:\n",
    "            lr_img, hr_img = self._aligned_random_crop(lr_img, hr_img, self.patch_size)\n",
    "\n",
    "        # To tensors\n",
    "        lr = pil_to_tensor(lr_img)  # (3, h, w), [0,1]\n",
    "        hr = pil_to_tensor(hr_img)  # (3, H, W), [0,1]\n",
    "\n",
    "        return {\n",
    "            \"lr\": lr,\n",
    "            \"hr\": hr,\n",
    "            \"filename\": hrp.name,\n",
    "            \"scale\": self.scale,\n",
    "        }\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282696b0",
   "metadata": {},
   "source": [
    "Quick smoke test to validate shapes, scale relationship, and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7217761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X4 sanity check\n",
    "ds_valid_x4 = DIV2KDataset(data_root=DATA_ROOT, split=\"valid\", scale=4)\n",
    "print(\"Pairs in valid X4:\", len(ds_valid_x4))\n",
    "\n",
    "sample = ds_valid_x4[0]\n",
    "print(\"filename:\", sample[\"filename\"])\n",
    "print(\"scale:\", sample[\"scale\"])\n",
    "print(\"LR shape:\", tuple(sample[\"lr\"].shape))   # (3, h, w)\n",
    "print(\"HR shape:\", tuple(sample[\"hr\"].shape))   # (3, H, W)\n",
    "\n",
    "# double-check the scale relationship numerically (H == h*scale, W == w*scale)\n",
    "_, h, w = sample[\"lr\"].shape\n",
    "_, H, W = sample[\"hr\"].shape\n",
    "print(\"H == h*scale ? \", H == h * sample[\"scale\"], \"(\", H, \"==\", h, \"*\", sample[\"scale\"], \")\")\n",
    "print(\"W == w*scale ? \", W == w * sample[\"scale\"], \"(\", W, \"==\", w, \"*\", sample[\"scale\"], \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb8aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader smoke test\n",
    "loader = DataLoader(ds_valid_x4, batch_size=2, shuffle=False, num_workers=0) # num_workers=0 keeps it simple; we can tune later.\n",
    "\n",
    "batch = next(iter(loader))\n",
    "print(\"Batch keys:\", list(batch.keys()))\n",
    "print(\"LR batch:\", tuple(batch[\"lr\"].shape))  # (B, 3, h, w)\n",
    "print(\"HR batch:\", tuple(batch[\"hr\"].shape))  # (B, 3, H, W)\n",
    "print(\"filenames:\", batch[\"filename\"])\n",
    "print(\"scales:\", batch[\"scale\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a8e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch extraction test\n",
    "ds = DIV2KDataset(DATA_ROOT, split=\"train\", scale=4, patch_size=128)\n",
    "s = ds[0]\n",
    "print(\"HR patch:\", s[\"hr\"].shape)  # (3, 128, 128)\n",
    "print(\"LR patch:\", s[\"lr\"].shape)  # (3, 32, 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PixelForge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b66ef1",
   "metadata": {},
   "source": [
    "### Imports + Generator (PixelShuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eccdd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "727a9d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, n_feats: int):\n",
    "        super().__init__()\n",
    "        # Residual block\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(n_feats, n_feats, 3, padding=1),  # First conv\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(n_feats, n_feats, 3, padding=1),  # Second conv\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec801c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleBlock(nn.Module):\n",
    "    \"\"\"X2 upsample with PixelShuffle; chain this block log2(scale) times.\"\"\"\n",
    "    def __init__(self, n_feats: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_feats, 4 * n_feats, 3, padding=1)\n",
    "        self.ps = nn.PixelShuffle(2)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.act(self.ps(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13a93829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpscaleGeneratorV0(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal upscaler:\n",
    "      - scale ∈ {2,4,8}\n",
    "      - alpha, preserve_graphics are placeholders for future steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, scale: int = 4, n_feats: int = 64, n_res: int = 16):\n",
    "        super().__init__()\n",
    "        assert scale in {2, 4, 8}, \"Scale must be 2, 4, or 8.\"\n",
    "        self.scale = scale\n",
    "\n",
    "        # Shallow feature extractor\n",
    "        self.head = nn.Conv2d(3, n_feats, 3, padding=1)\n",
    "\n",
    "        # Lightweight residual blocks\n",
    "        self.body = nn.Sequential(*[ResidualBlock(n_feats) for _ in range(n_res)])\n",
    "\n",
    "        # Upsampling blocks\n",
    "        up_blocks = []\n",
    "        steps = int(math.log2(scale))\n",
    "        for _ in range(steps):\n",
    "            up_blocks.append(UpsampleBlock(n_feats))\n",
    "        self.upsampler = nn.Sequential(*up_blocks)\n",
    "\n",
    "        # Reconstruction to RGB\n",
    "        self.tail = nn.Conv2d(n_feats, 3, 3, padding=1)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, alpha: float = 0.0, preserve_graphics: bool = False) -> torch.Tensor:\n",
    "        # Alpha and preserve_graphics are just used yet; just part of the API.\n",
    "        return self.tail(self.upsampler(self.body(self.head(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e6260c",
   "metadata": {},
   "source": [
    "Smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e056a054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR shape: (1, 3, 24, 24)\n",
      "SR shape: (1, 3, 96, 96)\n",
      "Trainable params (M): 0.594\n"
     ]
    }
   ],
   "source": [
    "# Repro seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Dummy LR image: 24x24 -> expect 96x96 for scale=4\n",
    "lr = torch.randn(1, 3, 24, 24)\n",
    "\n",
    "gen = UpscaleGeneratorV0(scale=4, n_feats=64, n_res=4)\n",
    "with torch.no_grad():\n",
    "    sr = gen(lr, alpha=0.0, preserve_graphics=False)\n",
    "\n",
    "print(\"LR shape:\", tuple(lr.shape))\n",
    "print(\"SR shape:\", tuple(sr.shape))\n",
    "print(\"Trainable params (M):\", round(gen.count_params() / 1e6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed5ed8",
   "metadata": {},
   "source": [
    "### α‑conditioned residual blocks (FiLM) + new generator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "239b11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaMLP(nn.Module):\n",
    "    \"\"\"Maps scalar alpha -> per-channel (gamma, beta) for FiLM.\"\"\"\n",
    "    def __init__(self, n_feats: int, hidden: int = 32, gain: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Create a simple 2-layer MLP:\n",
    "        # Input: alpha (single value)\n",
    "        # Hidden layer: 32 neurons by default\n",
    "        # Output: 2*n_feats (gamma and beta for each channel)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, hidden),          # First layer: 1 -> hidden_dim\n",
    "            nn.ReLU(inplace=True),         # Activation function\n",
    "            nn.Linear(hidden, 2 * n_feats), # Second layer: hidden_dim -> 2*n_feats\n",
    "        )\n",
    "        # Gain controls the strength of the modulation\n",
    "        # Lower gain = gentler modulation of features\n",
    "        # This prevents extreme values at the start of training\n",
    "        self.gain = gain\n",
    "\n",
    "    def forward(self, alpha: torch.Tensor):\n",
    "        \"\"\"\n",
    "        alpha: shape (N,) in [0,1]\n",
    "        returns gamma, beta with shape (N, C)\n",
    "        \"\"\"\n",
    "        # Handle scalar input by converting to a 1D tensor\n",
    "        if alpha.dim() == 0:\n",
    "            alpha = alpha.view(1)  # Convert scalar to 1D tensor\n",
    "            \n",
    "        # Add feature dimension for Linear layer (N,) -> (N,1)\n",
    "        out = self.net(alpha.unsqueeze(-1))  # Results in (N, 2*C)\n",
    "        \n",
    "        # Apply tanh to bound values between -1 and 1, then scale by gain\n",
    "        # This keeps the modulation gentle, especially early in training\n",
    "        out = torch.tanh(out) * self.gain\n",
    "        \n",
    "        # Split the output into gamma and beta components\n",
    "        # gamma is multiplicative scale, beta is additive shift\n",
    "        gamma, beta = out.chunk(2, dim=-1)  # Each is (N, C)\n",
    "        \n",
    "        return gamma, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad4580a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockCond(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional Residual Block that applies Feature-wise Linear Modulation (FiLM).\n",
    "    Unlike the basic ResidualBlock, this version can be modulated by external\n",
    "    parameters (gamma, beta) to conditionally adjust the feature representations.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_feats: int):\n",
    "        super().__init__()\n",
    "        # First convolution: maintains feature dimensions (n_feats -> n_feats)\n",
    "        # Uses 3x3 kernel with padding=1 to preserve spatial dimensions\n",
    "        self.conv1 = nn.Conv2d(n_feats, n_feats, 3, padding=1)\n",
    "        \n",
    "        # ReLU activation with inplace=True to save memory\n",
    "        # Applied between the two convolutions in the residual path\n",
    "        self.act   = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Second convolution: also maintains feature dimensions\n",
    "        # The output of this layer will be modulated by FiLM parameters\n",
    "        self.conv2 = nn.Conv2d(n_feats, n_feats, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, gamma: torch.Tensor, beta: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass with Feature-wise Linear Modulation (FiLM).\n",
    "        \n",
    "        Args:\n",
    "            x: Input feature tensor of shape (N, C, H, W)\n",
    "            gamma: Multiplicative modulation parameter of shape (N, C)\n",
    "                  Controls the scale/gain for each channel\n",
    "            beta: Additive modulation parameter of shape (N, C)\n",
    "                 Controls the bias/shift for each channel\n",
    "        \n",
    "        The FiLM transformation applies: output = (1 + gamma) * features + beta\n",
    "        This allows the alpha parameter to control how features are processed.\n",
    "        \"\"\"\n",
    "        # Apply first convolution to input features\n",
    "        y = self.conv1(x)\n",
    "        \n",
    "        # Apply ReLU activation (non-linearity)\n",
    "        y = self.act(y)\n",
    "        \n",
    "        # Apply second convolution - this output will be modulated\n",
    "        y = self.conv2(y)\n",
    "        \n",
    "        # Extract batch size and channel dimensions for reshaping\n",
    "        N, C, _, _ = y.shape\n",
    "        \n",
    "        # Apply FiLM modulation:\n",
    "        # - Reshape gamma from (N,C) to (N,C,1,1) for broadcasting across H,W dims\n",
    "        # - (1.0 + gamma) creates multiplicative modulation centered around 1.0\n",
    "        # - Adding beta provides additive bias for each channel\n",
    "        # This allows alpha to control the strength and direction of feature modulation\n",
    "        y = y * (1.0 + gamma.view(N, C, 1, 1)) + beta.view(N, C, 1, 1)\n",
    "        \n",
    "        # Residual connection: add original input to modulated features\n",
    "        # This preserves gradient flow and allows the network to learn identity mappings\n",
    "        return x + y\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling block that doubles spatial resolution using PixelShuffle.\n",
    "    This is a sub-pixel convolution approach that's more parameter efficient\n",
    "    than transposed convolutions and avoids checkerboard artifacts.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_feats: int):\n",
    "        super().__init__()\n",
    "        # Convolution that increases channels by 4x (for 2x2 PixelShuffle)\n",
    "        # Input: n_feats channels, Output: 4*n_feats channels\n",
    "        # The 4x factor comes from 2x2 upsampling (2*2=4 sub-pixels per pixel)\n",
    "        self.conv = nn.Conv2d(n_feats, 4 * n_feats, 3, padding=1)\n",
    "        \n",
    "        # PixelShuffle rearranges 4*n_feats channels into 2x larger spatial dims\n",
    "        # Takes (N, 4*C, H, W) -> (N, C, 2*H, 2*W)\n",
    "        # This is the core of sub-pixel convolution upsampling\n",
    "        self.ps = nn.PixelShuffle(2)\n",
    "        \n",
    "        # ReLU activation applied after upsampling\n",
    "        # Ensures positive feature values going into next layer\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: Conv -> PixelShuffle -> ReLU\n",
    "        Doubles the spatial resolution while maintaining channel count.\n",
    "        \"\"\"\n",
    "        # Apply convolution to increase channel count by 4x\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # Rearrange channels to increase spatial resolution by 2x\n",
    "        x = self.ps(x)\n",
    "        \n",
    "        # Apply activation for non-linearity\n",
    "        return self.act(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ba6543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpscaleGeneratorV1(nn.Module):\n",
    "    \"\"\"\n",
    "    V1: like V0 but residuals are modulated by alpha via FiLM.\n",
    "    API stays: forward(x, alpha, preserve_graphics)\n",
    "    \"\"\"\n",
    "    def __init__(self, scale: int = 4, n_feats: int = 64, n_res: int = 4):\n",
    "        super().__init__()\n",
    "        # Ensure scale is a power of 2 between 2 and 8\n",
    "        # This determines how many upsampling blocks we need\n",
    "        assert scale in (2,4,8)\n",
    "        self.scale = scale\n",
    "        self.n_feats = n_feats\n",
    "\n",
    "        # Initial feature extraction: RGB (3 channels) -> feature space (n_feats channels)\n",
    "        # This conv layer learns to extract low-level features from the input image\n",
    "        self.head = nn.Conv2d(3, n_feats, 3, padding=1)\n",
    "        \n",
    "        # Main processing backbone: stack of conditional residual blocks\n",
    "        # Using ModuleList allows us to iterate over blocks in forward pass\n",
    "        # Each block can be modulated differently by the alpha parameter via FiLM\n",
    "        self.body = nn.ModuleList([ResidualBlockCond(n_feats) for _ in range(n_res)])\n",
    "\n",
    "        # Upsampling path: chain multiple 2x upsampling blocks to reach target scale\n",
    "        # For scale=4: need 2 blocks (2^2=4), for scale=8: need 3 blocks (2^3=8)\n",
    "        steps = int(math.log2(scale))\n",
    "        self.upsampler = nn.Sequential(*[UpsampleBlock(n_feats) for _ in range(steps)])\n",
    "        \n",
    "        # Final reconstruction: convert features back to RGB space\n",
    "        # This is where the final super-resolved image is generated\n",
    "        self.tail = nn.Conv2d(n_feats, 3, 3, padding=1)\n",
    "\n",
    "        # Alpha MLP: converts scalar alpha value into FiLM parameters (gamma, beta)\n",
    "        # Shared across all residual blocks - one alpha controls the entire network\n",
    "        # Hidden=32: compact representation, gain=0.1: gentle modulation to start\n",
    "        self.alpha_mlp = AlphaMLP(n_feats, hidden=32, gain=0.1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def count_params(self):\n",
    "        # Utility method to count trainable parameters for model size analysis\n",
    "        # @torch.no_grad() prevents gradient tracking during parameter counting\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def forward(self, x, alpha: float = 0.0, preserve_graphics: bool = False):\n",
    "        # Convert alpha to tensor if it's a scalar, ensuring it's on the right device/dtype\n",
    "        # This handles both tensor and scalar inputs consistently\n",
    "        if not torch.is_tensor(alpha):\n",
    "            alpha = torch.tensor(alpha, dtype=x.dtype, device=x.device)\n",
    "        \n",
    "        # Defensive clamping: ensure alpha stays in valid [0,1] range\n",
    "        # Values outside this range could cause unstable FiLM modulation\n",
    "        alpha = torch.clamp(alpha, 0.0, 1.0)\n",
    "\n",
    "        # Extract initial features from RGB input\n",
    "        # Shape: (N, 3, H, W) -> (N, n_feats, H, W)\n",
    "        feat = self.head(x)\n",
    "\n",
    "        # Prepare FiLM parameters for all residual blocks\n",
    "        # Get batch size to ensure alpha is broadcast correctly across the batch\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # Expand alpha to match batch size: scalar -> (N,)\n",
    "        # This allows different alpha values per batch item if needed\n",
    "        a = alpha.expand(N)  # (N,)\n",
    "        \n",
    "        # Generate FiLM modulation parameters from alpha\n",
    "        # gamma: multiplicative scaling per channel, beta: additive bias per channel\n",
    "        # Both have shape (N, C) where C = n_feats\n",
    "        gamma, beta = self.alpha_mlp(a)  # (N,C),(N,C)\n",
    "\n",
    "        # Apply conditional residual blocks with FiLM modulation\n",
    "        # Each block receives the same gamma/beta but can use them differently\n",
    "        # The alpha parameter thus controls the processing style across all blocks\n",
    "        for block in self.body:\n",
    "            feat = block(feat, gamma, beta)\n",
    "\n",
    "        # Upsample features to target resolution\n",
    "        # Each UpsampleBlock doubles spatial dimensions: H,W -> 2H,2W\n",
    "        # Final spatial size will be original_size * scale\n",
    "        feat = self.upsampler(feat)\n",
    "        \n",
    "        # Convert upsampled features back to RGB image\n",
    "        # Shape: (N, n_feats, scale*H, scale*W) -> (N, 3, scale*H, scale*W)\n",
    "        out  = self.tail(feat)\n",
    "        \n",
    "        # Return the super-resolved image\n",
    "        # preserve_graphics parameter is not used yet but part of future API\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa49f017",
   "metadata": {},
   "source": [
    "Check: α actually changes the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77691be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR shape @alpha=0.0: (1, 3, 96, 96)\n",
      "Δ(0.0, 0.5) MAD: 0.003001\n",
      "Δ(0.5, 1.0) MAD: 0.002831\n",
      "Params (M): 0.599\n"
     ]
    }
   ],
   "source": [
    "# Dummy LR input\n",
    "torch.manual_seed(0)\n",
    "lr = torch.randn(1, 3, 24, 24)\n",
    "\n",
    "# Create V1 generator with alpha conditioning\n",
    "gen = UpscaleGeneratorV1(scale=4, n_feats=64, n_res=4)\n",
    "with torch.no_grad():\n",
    "    # Test different alpha values to see FiLM modulation effects\n",
    "    sr_a0 = gen(lr, alpha=0.0)  # No modulation (alpha=0)\n",
    "    sr_a5 = gen(lr, alpha=0.5)  # Medium modulation \n",
    "    sr_a1 = gen(lr, alpha=1.0)  # Full modulation (alpha=1)\n",
    "\n",
    "# Helper function to compute mean absolute difference\n",
    "def mad(a, b):  # mean absolute diff\n",
    "    return (a - b).abs().mean().item()\n",
    "\n",
    "# Report shapes and differences between alpha values\n",
    "print(\"SR shape @alpha=0.0:\", tuple(sr_a0.shape))\n",
    "print(\"Δ(0.0, 0.5) MAD:\", round(mad(sr_a0, sr_a5), 6))  # Difference between alpha=0 and 0.5\n",
    "print(\"Δ(0.5, 1.0) MAD:\", round(mad(sr_a5, sr_a1), 6))  # Difference between alpha=0.5 and 1.0\n",
    "print(\"Params (M):\", round(gen.count_params()/1e6, 3))  # Model size in millions of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70920ab8",
   "metadata": {},
   "source": [
    "### Add `preserve_graphics` hook (mask plumbing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae3062",
   "metadata": {},
   "source": [
    "Mask stub + integrate in forward (no effect yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3db0fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def structure_mask_v0(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Placeholder structure mask.\n",
    "    Returns a tensor of ones with shape (N,1,H,W) to keep behavior neutral.\n",
    "    Later we'll detect edges/text to prioritize them.\n",
    "    \"\"\"\n",
    "    # Extract batch size and spatial dimensions from input tensor\n",
    "    N, C, H, W = x.shape\n",
    "    # Return all-ones mask: no graphics preservation yet, just neutral passthrough\n",
    "    return torch.ones((N, 1, H, W), dtype=x.dtype, device=x.device)\n",
    "\n",
    "# Patch UpscaleGeneratorV1.forward to accept preserve_graphics and compute mask (no-op blend)\n",
    "def _forward_with_mask(self, x, alpha: float = 0.0, preserve_graphics: bool = False):\n",
    "    # ===== original body up to 'out' =====\n",
    "    # Ensure alpha is a tensor with proper dtype and device\n",
    "    if not torch.is_tensor(alpha):\n",
    "        alpha = torch.tensor(alpha, dtype=x.dtype, device=x.device)\n",
    "    # Clamp alpha to valid range [0,1] to prevent unstable FiLM modulation\n",
    "    alpha = torch.clamp(alpha, 0.0, 1.0)\n",
    "\n",
    "    # Extract initial features from RGB input using head convolution\n",
    "    feat = self.head(x)\n",
    "    # Get batch size for broadcasting alpha across batch dimension\n",
    "    N = x.shape[0]\n",
    "    # Expand scalar alpha to match batch size: scalar -> (N,)\n",
    "    a = alpha.expand(N)\n",
    "    # Generate FiLM parameters (gamma, beta) from alpha using MLP\n",
    "    gamma, beta = self.alpha_mlp(a)\n",
    "\n",
    "    # Apply conditional residual blocks with FiLM modulation\n",
    "    for block in self.body:\n",
    "        feat = block(feat, gamma, beta)\n",
    "\n",
    "    # Upsample features to target resolution using pixel shuffle blocks\n",
    "    feat = self.upsampler(feat)\n",
    "    # Convert upsampled features back to RGB space\n",
    "    out  = self.tail(feat)  # (N,3,Hs,Ws)\n",
    "\n",
    "    # ===== graphics-preservation plumbing (neutral for now) =====\n",
    "    if preserve_graphics:\n",
    "        # Generate structure mask for graphics preservation (currently neutral)\n",
    "        M = structure_mask_v0(out)  # (N,1,Hs,Ws)\n",
    "        # Neutral blend: y = out (since M=1 and we don't change anything)\n",
    "        # Future implementation will blend based on mask to preserve graphics\n",
    "        y = out\n",
    "        return y\n",
    "    else:\n",
    "        # Standard path: return upscaled output without graphics preservation\n",
    "        return out\n",
    "\n",
    "# Monkey-patch method (keeps notebook simple)\n",
    "# Replace the forward method of UpscaleGeneratorV1 with our enhanced version\n",
    "UpscaleGeneratorV1.forward = _forward_with_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed75f8f",
   "metadata": {},
   "source": [
    "##### Testing Graphics Preservation Feature\n",
    "\n",
    "We'll test the newly added `preserve_graphics` parameter to verify that:\n",
    "1. The mask generation function works correctly\n",
    "2. The graphics preservation pipeline is properly integrated\n",
    "3. Currently it should behave as a no-op (neutral effect) since the mask returns all ones\n",
    "\n",
    "The test will compare outputs with and without `preserve_graphics=True` to confirm they are identical, validating that our plumbing is correct before implementing actual graphics detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a0cb23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR shape: (1, 3, 96, 96)\n",
      "MAD (graphics off vs on): 0.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)  # set random seed for reproducible tensor draws\n",
    "lr = torch.randn(1, 3, 24, 24)  # create a dummy low-resolution image tensor (N=1, C=3, H=24, W=24)\n",
    "\n",
    "gen = UpscaleGeneratorV1(scale=4, n_feats=64, n_res=4)  # instantiate the alpha‑conditioned upscaler\n",
    "with torch.no_grad():  # disable gradient tracking for inference\n",
    "    sr_off = gen(lr, alpha=0.3, preserve_graphics=False)  # run generator without graphics preservation\n",
    "    sr_on  = gen(lr, alpha=0.3, preserve_graphics=True)   # run generator with graphics preservation enabled\n",
    "\n",
    "mad = (sr_off - sr_on).abs().mean().item()  # compute mean absolute difference between the two outputs\n",
    "print(\"SR shape:\", tuple(sr_on.shape))  # print shape of super-resolved output\n",
    "print(\"MAD (graphics off vs on):\", mad)  # print the mean absolute difference (should be 0 for neutral mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ebe4d",
   "metadata": {},
   "source": [
    "### Notebook micro-train (GPU availability sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98f35976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.5.1\n",
      "CUDA available: True\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Small, fast settings so we can iterate\n",
    "SCALE = 4            # {2,4,8}\n",
    "BATCH_SIZE = 4\n",
    "NUM_STEPS = 50       # tiny smoke run\n",
    "LR = 2e-4\n",
    "ALPHA = 0.0          # start fully faithful; we'll use the knob later\n",
    "IMG_SIZE_HR = 96     # small patches -> faster\n",
    "\n",
    "# Windows-friendly DataLoader defaults\n",
    "NUM_WORKERS = 0      # Avoid issues on Windows\n",
    "PIN_MEMORY = True    # helps GPU batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21255f17",
   "metadata": {},
   "source": [
    "### Dataset Configuration and Data Loading Pipeline\n",
    "\n",
    "This section sets up the training dataset and data loader infrastructure:\n",
    "- Configures paths for high-resolution training images\n",
    "- Creates on-the-fly paired dataset (HR → LR via bicubic downsampling)\n",
    "- Establishes data loading pipeline with batching and GPU optimization\n",
    "- Handles fallback directories and validates data availability for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e31a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".webp\"}\n",
    "\n",
    "class PairedOnTheFlyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads HR images, creates LR via bicubic downscale by SCALE.\n",
    "    Returns: {'lr': tensor, 'hr': tensor, 'path': str}\n",
    "    \"\"\"\n",
    "    def __init__(self, root_hr: Path, fallback: Path | None, scale: int = 4, patch_hr: int | None = None):\n",
    "        self.scale = scale\n",
    "        self.patch_hr = patch_hr\n",
    "        self.root_hr = Path(root_hr)\n",
    "        self.paths = []\n",
    "        if self.root_hr.exists():\n",
    "            self.paths = [p for p in self.root_hr.rglob(\"*\") if p.suffix.lower() in IMG_EXTS]\n",
    "        if not self.paths and fallback and Path(fallback).exists():\n",
    "            self.paths = [p for p in Path(fallback).rglob(\"*\") if p.suffix.lower() in IMG_EXTS]\n",
    "        if not self.paths:\n",
    "            raise FileNotFoundError(\"No images found in data/HR/ or data/samples/. Add a few images to proceed.\")\n",
    "\n",
    "    def _center_crop_multiple(self, img: Image.Image, multiple: int) -> Image.Image:\n",
    "        w, h = img.size\n",
    "        W = (w // multiple) * multiple\n",
    "        H = (h // multiple) * multiple\n",
    "        left = (w - W) // 2\n",
    "        top  = (h - H) // 2\n",
    "        return img.crop((left, top, left + W, top + H))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        hr = Image.open(path).convert(\"RGB\")\n",
    "        hr = self._center_crop_multiple(hr, self.scale)\n",
    "\n",
    "        # Optional HR patch for speed\n",
    "        if self.patch_hr is not None:\n",
    "            w, h = hr.size\n",
    "            if w >= self.patch_hr and h >= self.patch_hr:\n",
    "                left = (w - self.patch_hr) // 2\n",
    "                top  = (h - self.patch_hr) // 2\n",
    "                hr = hr.crop((left, top, left + self.patch_hr, top + self.patch_hr))\n",
    "\n",
    "        # Make LR via bicubic\n",
    "        w, h = hr.size\n",
    "        lr = hr.resize((w // self.scale, h // self.scale), Image.BICUBIC)\n",
    "\n",
    "        # To tensors in [0,1]\n",
    "        hr_t = TF.to_tensor(hr)\n",
    "        lr_t = TF.to_tensor(lr)\n",
    "        return {\"lr\": lr_t, \"hr\": hr_t, \"path\": str(path)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7b6bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_ROOT = Path(\"..\\\\data\")\n",
    "HR_VALID = DATA_ROOT / \"HR\" / \"valid\"\n",
    "HR_TRAIN = DATA_ROOT / \"HR\" / \"train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a9b42a",
   "metadata": {},
   "source": [
    "#### DataLoader + model to GPU\n",
    "We'll reuse `UpscaleGeneratorV1` from earlier in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ab1a9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params (M): 0.599\n"
     ]
    }
   ],
   "source": [
    "# Build dataset & loader\n",
    "root_hr = Path(\"..\\\\data\") / \"HR\"\n",
    "fallback = Path(\"..\\\\data\") / \"samples\"\n",
    "train_ds = PairedOnTheFlyDataset(root_hr, fallback, scale=SCALE, patch_hr=IMG_SIZE_HR)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=True)\n",
    "\n",
    "# Grab model from previous cell; if needed, re-run its definition.\n",
    "gen = UpscaleGeneratorV1(scale=SCALE, n_feats=64, n_res=4).to(device)\n",
    "print(\"Params (M):\", round(gen.count_params()/1e6, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb9321a",
   "metadata": {},
   "source": [
    "##### Tiny AMP training loop (L1 only, α=0 for now)\n",
    "We’ll do a single-network (generator) optimization against an L1 reconstruction loss. This is not GAN training yet; it’s just to validate the pipeline, GPU, dataloader, and that loss decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37bf3913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 000  L1: 0.5374\n",
      "step 010  L1: 0.1424\n",
      "step 020  L1: 0.1761\n",
      "step 030  L1: 0.1041\n",
      "step 040  L1: 0.0865\n",
      "done 50 steps in 12.64s ; last L1=0.1017\n",
      "mean L1: 0.16555019304156304\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast, GradScaler  # Updated import path for newer PyTorch versions\n",
    "from time import perf_counter\n",
    "\n",
    "# Set model to training mode (enables dropout, batch norm training behavior)\n",
    "gen.train()\n",
    "\n",
    "# Initialize Adam optimizer with specified learning rate and beta parameters\n",
    "# betas=(0.9, 0.99) are slightly different from default (0.9, 0.999) for stability\n",
    "opt = torch.optim.Adam(gen.parameters(), lr=LR, betas=(0.9, 0.99))\n",
    "\n",
    "# Initialize gradient scaler for automatic mixed precision (AMP) training\n",
    "# Only enabled when using CUDA to avoid potential issues on CPU\n",
    "scaler = GradScaler(device=device.type)\n",
    "\n",
    "# Training loop initialization\n",
    "step = 0  # Current training step counter\n",
    "t0 = perf_counter()  # Start time for measuring training duration\n",
    "loss_hist = []  # List to store loss values for analysis\n",
    "\n",
    "# Main training loop over batches\n",
    "for batch in train_dl:\n",
    "    # Stop training after reaching the specified number of steps\n",
    "    if step >= NUM_STEPS:\n",
    "        break\n",
    "    \n",
    "    # Move input tensors to GPU with non-blocking transfer for efficiency\n",
    "    # non_blocking=True allows CPU-GPU transfer to overlap with computation\n",
    "    lr = batch[\"lr\"].to(device, non_blocking=True)  # Low-resolution input images\n",
    "    hr = batch[\"hr\"].to(device, non_blocking=True)  # High-resolution target images\n",
    "\n",
    "    # Zero gradients from previous iteration\n",
    "    # set_to_none=True is more memory efficient than setting to zero\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # Forward pass with automatic mixed precision\n",
    "    # autocast automatically uses float16 for compatible operations to save memory/speed\n",
    "    with autocast(device_type=device.type):\n",
    "        # Generate super-resolved image using current alpha value and no graphics preservation\n",
    "        sr = gen(lr, alpha=ALPHA, preserve_graphics=False)\n",
    "        \n",
    "        # Compute L1 (Mean Absolute Error) loss between generated and target images\n",
    "        # L1 loss encourages sharp reconstruction and is commonly used in super-resolution\n",
    "        loss = F.l1_loss(sr, hr)\n",
    "\n",
    "    # Backward pass with gradient scaling for mixed precision training\n",
    "    # scaler.scale() scales the loss to prevent gradient underflow in float16\n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "    # Optimizer step with gradient unscaling and clipping if needed\n",
    "    # scaler.step() unscales gradients before applying them\n",
    "    scaler.step(opt)\n",
    "    \n",
    "    # Update the gradient scaler's internal state for next iteration\n",
    "    # Adjusts scaling factor based on whether gradients were finite\n",
    "    scaler.update()\n",
    "\n",
    "    # Log training progress\n",
    "    loss_hist.append(loss.item())  # Store loss value for later analysis\n",
    "    \n",
    "    # Print progress every 10 steps\n",
    "    if step % 10 == 0:\n",
    "        print(f\"step {step:03d}  L1: {loss.item():.4f}\")\n",
    "    \n",
    "    step += 1  # Increment step counter\n",
    "\n",
    "# Training completion summary\n",
    "t1 = perf_counter()  # End time\n",
    "print(f\"done {step} steps in {t1 - t0:.2f}s ; last L1={loss_hist[-1]:.4f}\")\n",
    "print(\"mean L1:\", sum(loss_hist)/len(loss_hist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e96e5c",
   "metadata": {},
   "source": [
    "Save checkpoint for continuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03285172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: \\PixelForge\\experiments\\checkpoints\\gen_v1_smoke.pt\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = Path(\"..\\\\experiments\") / \"checkpoints\"\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "ckpt_path = ckpt_dir / \"gen_v1_smoke.pt\"\n",
    "\n",
    "torch.save({\n",
    "    \"model\": gen.state_dict(),\n",
    "    \"scale\": SCALE,\n",
    "    \"alpha_hint\": ALPHA,\n",
    "    \"meta\": {\"steps\": step, \"img_size_hr\": IMG_SIZE_HR}\n",
    "}, ckpt_path)\n",
    "\n",
    "print(\"Saved:\", ckpt_path.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PixelForge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

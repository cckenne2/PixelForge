{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b66ef1",
   "metadata": {},
   "source": [
    "### Imports + Generator (PixelShuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eccdd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# GPU setup and verification\n",
    "def check_gpu_setup():\n",
    "    \"\"\"Check CUDA availability and GPU memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        current_gpu = torch.cuda.current_device()\n",
    "        gpu_name = torch.cuda.get_device_name(current_gpu)\n",
    "        gpu_memory = torch.cuda.get_device_properties(current_gpu).total_memory / 1024**3\n",
    "        print(f\"✓ CUDA available with {gpu_count} GPU(s)\")\n",
    "        print(f\"✓ Using GPU {current_gpu}: {gpu_name}\")\n",
    "        print(f\"✓ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"⚠ CUDA not available, falling back to CPU\")\n",
    "        return False\n",
    "\n",
    "# Initialize GPU optimizations\n",
    "def optimize_gpu_settings():\n",
    "    \"\"\"Optimize GPU settings for better performance\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Enable cuDNN optimizations\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"✓ GPU optimizations enabled\")\n",
    "        print(f\"✓ cuDNN benchmark mode: {torch.backends.cudnn.benchmark}\")\n",
    "    else:\n",
    "        print(\"⚠ No CUDA GPU available for optimization\")\n",
    "\n",
    "# Check GPU setup and optimize\n",
    "CUDA_AVAILABLE = check_gpu_setup()\n",
    "optimize_gpu_settings()\n",
    "\n",
    "# Set default device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Default device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727a9d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, n_feats: int):\n",
    "        super().__init__()\n",
    "        # Residual block\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(n_feats, n_feats, 3, padding=1),  # First conv\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(n_feats, n_feats, 3, padding=1),  # Second conv\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec801c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleBlock(nn.Module):\n",
    "    \"\"\"X2 upsample with PixelShuffle; chain this block log2(scale) times.\"\"\"\n",
    "    def __init__(self, n_feats: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_feats, 4 * n_feats, 3, padding=1)\n",
    "        self.ps = nn.PixelShuffle(2)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.act(self.ps(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a93829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpscaleGeneratorV0(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal upscaler:\n",
    "      - scale ∈ {2,4,8}\n",
    "      - alpha, preserve_graphics are placeholders for future steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, scale: int = 4, n_feats: int = 64, n_res: int = 16):\n",
    "        super().__init__()\n",
    "        assert scale in {2, 4, 8}, \"Scale must be 2, 4, or 8.\"\n",
    "        self.scale = scale\n",
    "\n",
    "        # Shallow feature extractor\n",
    "        self.head = nn.Conv2d(3, n_feats, 3, padding=1)\n",
    "\n",
    "        # Lightweight residual blocks\n",
    "        self.body = nn.Sequential(*[ResidualBlock(n_feats) for _ in range(n_res)])\n",
    "\n",
    "        # Upsampling blocks\n",
    "        up_blocks = []\n",
    "        steps = int(math.log2(scale))\n",
    "        for _ in range(steps):\n",
    "            up_blocks.append(UpsampleBlock(n_feats))\n",
    "        self.upsampler = nn.Sequential(*up_blocks)\n",
    "\n",
    "        # Reconstruction to RGB\n",
    "        self.tail = nn.Conv2d(n_feats, 3, 3, padding=1)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, alpha: float = 0.0, preserve_graphics: bool = False) -> torch.Tensor:\n",
    "        # Alpha and preserve_graphics are just used yet; just part of the API.\n",
    "        return self.tail(self.upsampler(self.body(self.head(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e6260c",
   "metadata": {},
   "source": [
    "Smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e056a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repro seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Dummy LR image: 24x24 -> expect 96x96 for scale=4\n",
    "lr = torch.randn(1, 3, 24, 24).to(device)  # Move to GPU\n",
    "\n",
    "gen = UpscaleGeneratorV0(scale=4, n_feats=64, n_res=4).to(device)  # Move model to GPU\n",
    "with torch.no_grad():\n",
    "    sr = gen(lr, alpha=0.0, preserve_graphics=False)\n",
    "\n",
    "print(\"LR shape:\", tuple(lr.shape))\n",
    "print(\"SR shape:\", tuple(sr.shape))\n",
    "print(\"Device:\", lr.device)\n",
    "print(\"Model device:\", next(gen.parameters()).device)\n",
    "print(\"Trainable params (M):\", round(gen.count_params() / 1e6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed5ed8",
   "metadata": {},
   "source": [
    "### α‑conditioned residual blocks (FiLM) + new generator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239b11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaMLP(nn.Module):\n",
    "    \"\"\"Maps scalar alpha -> per-channel (gamma, beta) for FiLM.\"\"\"\n",
    "    def __init__(self, n_feats: int, hidden: int = 32, gain: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Create a simple 2-layer MLP:\n",
    "        # Input: alpha (single value)\n",
    "        # Hidden layer: 32 neurons by default\n",
    "        # Output: 2*n_feats (gamma and beta for each channel)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, hidden),          # First layer: 1 -> hidden_dim\n",
    "            nn.ReLU(inplace=True),         # Activation function\n",
    "            nn.Linear(hidden, 2 * n_feats), # Second layer: hidden_dim -> 2*n_feats\n",
    "        )\n",
    "        # Gain controls the strength of the modulation\n",
    "        # Lower gain = gentler modulation of features\n",
    "        # This prevents extreme values at the start of training\n",
    "        self.gain = gain\n",
    "\n",
    "    def forward(self, alpha: torch.Tensor):\n",
    "        \"\"\"\n",
    "        alpha: shape (N,) in [0,1]\n",
    "        returns gamma, beta with shape (N, C)\n",
    "        \"\"\"\n",
    "        # Handle scalar input by converting to a 1D tensor\n",
    "        if alpha.dim() == 0:\n",
    "            alpha = alpha.view(1)  # Convert scalar to 1D tensor\n",
    "            \n",
    "        # Add feature dimension for Linear layer (N,) -> (N,1)\n",
    "        out = self.net(alpha.unsqueeze(-1))  # Results in (N, 2*C)\n",
    "        \n",
    "        # Apply tanh to bound values between -1 and 1, then scale by gain\n",
    "        # This keeps the modulation gentle, especially early in training\n",
    "        out = torch.tanh(out) * self.gain\n",
    "        \n",
    "        # Split the output into gamma and beta components\n",
    "        # gamma is multiplicative scale, beta is additive shift\n",
    "        gamma, beta = out.chunk(2, dim=-1)  # Each is (N, C)\n",
    "        \n",
    "        return gamma, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4580a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockCond(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional Residual Block that applies Feature-wise Linear Modulation (FiLM).\n",
    "    Unlike the basic ResidualBlock, this version can be modulated by external\n",
    "    parameters (gamma, beta) to conditionally adjust the feature representations.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_feats: int):\n",
    "        super().__init__()\n",
    "        # First convolution: maintains feature dimensions (n_feats -> n_feats)\n",
    "        # Uses 3x3 kernel with padding=1 to preserve spatial dimensions\n",
    "        self.conv1 = nn.Conv2d(n_feats, n_feats, 3, padding=1)\n",
    "        \n",
    "        # ReLU activation with inplace=True to save memory\n",
    "        # Applied between the two convolutions in the residual path\n",
    "        self.act   = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Second convolution: also maintains feature dimensions\n",
    "        # The output of this layer will be modulated by FiLM parameters\n",
    "        self.conv2 = nn.Conv2d(n_feats, n_feats, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, gamma: torch.Tensor, beta: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass with Feature-wise Linear Modulation (FiLM).\n",
    "        \n",
    "        Args:\n",
    "            x: Input feature tensor of shape (N, C, H, W)\n",
    "            gamma: Multiplicative modulation parameter of shape (N, C)\n",
    "                  Controls the scale/gain for each channel\n",
    "            beta: Additive modulation parameter of shape (N, C)\n",
    "                 Controls the bias/shift for each channel\n",
    "        \n",
    "        The FiLM transformation applies: output = (1 + gamma) * features + beta\n",
    "        This allows the alpha parameter to control how features are processed.\n",
    "        \"\"\"\n",
    "        # Apply first convolution to input features\n",
    "        y = self.conv1(x)\n",
    "        \n",
    "        # Apply ReLU activation (non-linearity)\n",
    "        y = self.act(y)\n",
    "        \n",
    "        # Apply second convolution - this output will be modulated\n",
    "        y = self.conv2(y)\n",
    "        \n",
    "        # Extract batch size and channel dimensions for reshaping\n",
    "        N, C, _, _ = y.shape\n",
    "        \n",
    "        # Apply FiLM modulation:\n",
    "        # - Reshape gamma from (N,C) to (N,C,1,1) for broadcasting across H,W dims\n",
    "        # - (1.0 + gamma) creates multiplicative modulation centered around 1.0\n",
    "        # - Adding beta provides additive bias for each channel\n",
    "        # This allows alpha to control the strength and direction of feature modulation\n",
    "        y = y * (1.0 + gamma.view(N, C, 1, 1)) + beta.view(N, C, 1, 1)\n",
    "        \n",
    "        # Residual connection: add original input to modulated features\n",
    "        # This preserves gradient flow and allows the network to learn identity mappings\n",
    "        return x + y\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling block that doubles spatial resolution using PixelShuffle.\n",
    "    This is a sub-pixel convolution approach that's more parameter efficient\n",
    "    than transposed convolutions and avoids checkerboard artifacts.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_feats: int):\n",
    "        super().__init__()\n",
    "        # Convolution that increases channels by 4x (for 2x2 PixelShuffle)\n",
    "        # Input: n_feats channels, Output: 4*n_feats channels\n",
    "        # The 4x factor comes from 2x2 upsampling (2*2=4 sub-pixels per pixel)\n",
    "        self.conv = nn.Conv2d(n_feats, 4 * n_feats, 3, padding=1)\n",
    "        \n",
    "        # PixelShuffle rearranges 4*n_feats channels into 2x larger spatial dims\n",
    "        # Takes (N, 4*C, H, W) -> (N, C, 2*H, 2*W)\n",
    "        # This is the core of sub-pixel convolution upsampling\n",
    "        self.ps = nn.PixelShuffle(2)\n",
    "        \n",
    "        # ReLU activation applied after upsampling\n",
    "        # Ensures positive feature values going into next layer\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: Conv -> PixelShuffle -> ReLU\n",
    "        Doubles the spatial resolution while maintaining channel count.\n",
    "        \"\"\"\n",
    "        # Apply convolution to increase channel count by 4x\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # Rearrange channels to increase spatial resolution by 2x\n",
    "        x = self.ps(x)\n",
    "        \n",
    "        # Apply activation for non-linearity\n",
    "        return self.act(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba6543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpscaleGeneratorV1(nn.Module):\n",
    "    \"\"\"\n",
    "    V1: like V0 but residuals are modulated by alpha via FiLM.\n",
    "    API stays: forward(x, alpha, preserve_graphics)\n",
    "    \"\"\"\n",
    "    def __init__(self, scale: int = 4, n_feats: int = 64, n_res: int = 4):\n",
    "        super().__init__()\n",
    "        # Ensure scale is a power of 2 between 2 and 8\n",
    "        # This determines how many upsampling blocks we need\n",
    "        assert scale in (2,4,8)\n",
    "        self.scale = scale\n",
    "        self.n_feats = n_feats\n",
    "\n",
    "        # Initial feature extraction: RGB (3 channels) -> feature space (n_feats channels)\n",
    "        # This conv layer learns to extract low-level features from the input image\n",
    "        self.head = nn.Conv2d(3, n_feats, 3, padding=1)\n",
    "        \n",
    "        # Main processing backbone: stack of conditional residual blocks\n",
    "        # Using ModuleList allows us to iterate over blocks in forward pass\n",
    "        # Each block can be modulated differently by the alpha parameter via FiLM\n",
    "        self.body = nn.ModuleList([ResidualBlockCond(n_feats) for _ in range(n_res)])\n",
    "\n",
    "        # Upsampling path: chain multiple 2x upsampling blocks to reach target scale\n",
    "        # For scale=4: need 2 blocks (2^2=4), for scale=8: need 3 blocks (2^3=8)\n",
    "        steps = int(math.log2(scale))\n",
    "        self.upsampler = nn.Sequential(*[UpsampleBlock(n_feats) for _ in range(steps)])\n",
    "        \n",
    "        # Final reconstruction: convert features back to RGB space\n",
    "        # This is where the final super-resolved image is generated\n",
    "        self.tail = nn.Conv2d(n_feats, 3, 3, padding=1)\n",
    "\n",
    "        # Alpha MLP: converts scalar alpha value into FiLM parameters (gamma, beta)\n",
    "        # Shared across all residual blocks - one alpha controls the entire network\n",
    "        # Hidden=32: compact representation, gain=0.1: gentle modulation to start\n",
    "        self.alpha_mlp = AlphaMLP(n_feats, hidden=32, gain=0.1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def count_params(self):\n",
    "        # Utility method to count trainable parameters for model size analysis\n",
    "        # @torch.no_grad() prevents gradient tracking during parameter counting\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def forward(self, x, alpha: float = 0.0, preserve_graphics: bool = False):\n",
    "        # Convert alpha to tensor if it's a scalar, ensuring it's on the right device/dtype\n",
    "        # This handles both tensor and scalar inputs consistently\n",
    "        if not torch.is_tensor(alpha):\n",
    "            alpha = torch.tensor(alpha, dtype=x.dtype, device=x.device)\n",
    "        \n",
    "        # Defensive clamping: ensure alpha stays in valid [0,1] range\n",
    "        # Values outside this range could cause unstable FiLM modulation\n",
    "        alpha = torch.clamp(alpha, 0.0, 1.0)\n",
    "\n",
    "        # Extract initial features from RGB input\n",
    "        # Shape: (N, 3, H, W) -> (N, n_feats, H, W)\n",
    "        feat = self.head(x)\n",
    "\n",
    "        # Prepare FiLM parameters for all residual blocks\n",
    "        # Get batch size to ensure alpha is broadcast correctly across the batch\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # Expand alpha to match batch size: scalar -> (N,)\n",
    "        # This allows different alpha values per batch item if needed\n",
    "        a = alpha.expand(N)  # (N,)\n",
    "        \n",
    "        # Generate FiLM modulation parameters from alpha\n",
    "        # gamma: multiplicative scaling per channel, beta: additive bias per channel\n",
    "        # Both have shape (N, C) where C = n_feats\n",
    "        gamma, beta = self.alpha_mlp(a)  # (N,C),(N,C)\n",
    "\n",
    "        # Apply conditional residual blocks with FiLM modulation\n",
    "        # Each block receives the same gamma/beta but can use them differently\n",
    "        # The alpha parameter thus controls the processing style across all blocks\n",
    "        for block in self.body:\n",
    "            feat = block(feat, gamma, beta)\n",
    "\n",
    "        # Upsample features to target resolution\n",
    "        # Each UpsampleBlock doubles spatial dimensions: H,W -> 2H,2W\n",
    "        # Final spatial size will be original_size * scale\n",
    "        feat = self.upsampler(feat)\n",
    "        \n",
    "        # Convert upsampled features back to RGB image\n",
    "        # Shape: (N, n_feats, scale*H, scale*W) -> (N, 3, scale*H, scale*W)\n",
    "        out  = self.tail(feat)\n",
    "        \n",
    "        # Return the super-resolved image\n",
    "        # preserve_graphics parameter is not used yet but part of future API\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa49f017",
   "metadata": {},
   "source": [
    "Check: α actually changes the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77691be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy LR input\n",
    "torch.manual_seed(0)\n",
    "lr = torch.randn(1, 3, 24, 24).to(device)  # Move to GPU\n",
    "\n",
    "# Create V1 generator with alpha conditioning and move to GPU\n",
    "gen = UpscaleGeneratorV1(scale=4, n_feats=64, n_res=4).to(device)\n",
    "with torch.no_grad():\n",
    "    # Test different alpha values to see FiLM modulation effects\n",
    "    sr_a0 = gen(lr, alpha=0.0)  # No modulation (alpha=0)\n",
    "    sr_a5 = gen(lr, alpha=0.5)  # Medium modulation \n",
    "    sr_a1 = gen(lr, alpha=1.0)  # Full modulation (alpha=1)\n",
    "\n",
    "# Helper function to compute mean absolute difference\n",
    "def mad(a, b):  # mean absolute diff\n",
    "    return (a - b).abs().mean().item()\n",
    "\n",
    "# Report shapes and differences between alpha values\n",
    "print(\"SR shape @alpha=0.0:\", tuple(sr_a0.shape))\n",
    "print(\"Δ(0.0, 0.5) MAD:\", round(mad(sr_a0, sr_a5), 6))  # Difference between alpha=0 and 0.5\n",
    "print(\"Δ(0.5, 1.0) MAD:\", round(mad(sr_a5, sr_a1), 6))  # Difference between alpha=0.5 and 1.0\n",
    "print(\"Params (M):\", round(gen.count_params()/1e6, 3))  # Model size in millions of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70920ab8",
   "metadata": {},
   "source": [
    "### Add `preserve_graphics` hook (mask plumbing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae3062",
   "metadata": {},
   "source": [
    "Mask stub + integrate in forward (no effect yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db0fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def structure_mask_v0(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Placeholder structure mask.\n",
    "    Returns a tensor of ones with shape (N,1,H,W) to keep behavior neutral.\n",
    "    Later we'll detect edges/text to prioritize them.\n",
    "    \"\"\"\n",
    "    # Extract batch size and spatial dimensions from input tensor\n",
    "    N, C, H, W = x.shape\n",
    "    # Return all-ones mask: no graphics preservation yet, just neutral passthrough\n",
    "    return torch.ones((N, 1, H, W), dtype=x.dtype, device=x.device)\n",
    "\n",
    "# Patch UpscaleGeneratorV1.forward to accept preserve_graphics and compute mask (no-op blend)\n",
    "def _forward_with_mask(self, x, alpha: float = 0.0, preserve_graphics: bool = False):\n",
    "    # ===== original body up to 'out' =====\n",
    "    # Ensure alpha is a tensor with proper dtype and device\n",
    "    if not torch.is_tensor(alpha):\n",
    "        alpha = torch.tensor(alpha, dtype=x.dtype, device=x.device)\n",
    "    # Clamp alpha to valid range [0,1] to prevent unstable FiLM modulation\n",
    "    alpha = torch.clamp(alpha, 0.0, 1.0)\n",
    "\n",
    "    # Extract initial features from RGB input using head convolution\n",
    "    feat = self.head(x)\n",
    "    # Get batch size for broadcasting alpha across batch dimension\n",
    "    N = x.shape[0]\n",
    "    # Expand scalar alpha to match batch size: scalar -> (N,)\n",
    "    a = alpha.expand(N)\n",
    "    # Generate FiLM parameters (gamma, beta) from alpha using MLP\n",
    "    gamma, beta = self.alpha_mlp(a)\n",
    "\n",
    "    # Apply conditional residual blocks with FiLM modulation\n",
    "    for block in self.body:\n",
    "        feat = block(feat, gamma, beta)\n",
    "\n",
    "    # Upsample features to target resolution using pixel shuffle blocks\n",
    "    feat = self.upsampler(feat)\n",
    "    # Convert upsampled features back to RGB space\n",
    "    out  = self.tail(feat)  # (N,3,Hs,Ws)\n",
    "\n",
    "    # ===== graphics-preservation plumbing (neutral for now) =====\n",
    "    if preserve_graphics:\n",
    "        # Generate structure mask for graphics preservation (currently neutral)\n",
    "        M = structure_mask_v0(out)  # (N,1,Hs,Ws)\n",
    "        # Neutral blend: y = out (since M=1 and we don't change anything)\n",
    "        # Future implementation will blend based on mask to preserve graphics\n",
    "        y = out\n",
    "        return y\n",
    "    else:\n",
    "        # Standard path: return upscaled output without graphics preservation\n",
    "        return out\n",
    "\n",
    "# Monkey-patch method (keeps notebook simple)\n",
    "# Replace the forward method of UpscaleGeneratorV1 with our enhanced version\n",
    "UpscaleGeneratorV1.forward = _forward_with_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed75f8f",
   "metadata": {},
   "source": [
    "##### Testing Graphics Preservation Feature\n",
    "\n",
    "We'll test the newly added `preserve_graphics` parameter to verify that:\n",
    "1. The mask generation function works correctly\n",
    "2. The graphics preservation pipeline is properly integrated\n",
    "3. Currently it should behave as a no-op (neutral effect) since the mask returns all ones\n",
    "\n",
    "The test will compare outputs with and without `preserve_graphics=True` to confirm they are identical, validating that our plumbing is correct before implementing actual graphics detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0cb23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)  # set random seed for reproducible tensor draws\n",
    "lr = torch.randn(1, 3, 24, 24).to(device)  # create a dummy low-resolution image tensor (N=1, C=3, H=24, W=24) and move to GPU\n",
    "\n",
    "gen = UpscaleGeneratorV1(scale=4, n_feats=64, n_res=4).to(device)  # instantiate the alpha‑conditioned upscaler and move to GPU\n",
    "with torch.no_grad():  # disable gradient tracking for inference\n",
    "    sr_off = gen(lr, alpha=0.3, preserve_graphics=False)  # run generator without graphics preservation\n",
    "    sr_on  = gen(lr, alpha=0.3, preserve_graphics=True)   # run generator with graphics preservation enabled\n",
    "\n",
    "mad = (sr_off - sr_on).abs().mean().item()  # compute mean absolute difference between the two outputs\n",
    "print(\"SR shape:\", tuple(sr_on.shape))  # print shape of super-resolved output\n",
    "print(\"MAD (graphics off vs on):\", mad)  # print the mean absolute difference (should be 0 for neutral mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ebe4d",
   "metadata": {},
   "source": [
    "### Notebook micro-train (GPU availability sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f35976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# GPU memory monitoring\n",
    "def monitor_gpu_memory():\n",
    "    \"\"\"Monitor and display current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f}GB | Reserved: {reserved:.2f}GB | Total: {total:.1f}GB\")\n",
    "        print(f\"Memory utilization: {(allocated/total)*100:.1f}%\")\n",
    "    else:\n",
    "        print(\"No CUDA GPU available for monitoring\")\n",
    "\n",
    "# Adaptive batch size based on GPU memory\n",
    "def get_optimal_batch_size():\n",
    "    \"\"\"Determine optimal batch size based on GPU memory\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return 2\n",
    "    \n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    \n",
    "    # Estimate batch size based on available memory\n",
    "    if gpu_memory_gb >= 16:\n",
    "        return 8\n",
    "    elif gpu_memory_gb >= 8:\n",
    "        return 6\n",
    "    elif gpu_memory_gb >= 4:\n",
    "        return 4\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# Training configuration optimized for GPU\n",
    "SCALE = 4                              # {2,4,8}\n",
    "BATCH_SIZE = get_optimal_batch_size()  # Dynamic batch size based on GPU memory\n",
    "NUM_STEPS = 100                        # Increased for better GPU utilization\n",
    "LR = 2e-4\n",
    "ALPHA = 0.0                           # start fully faithful; we'll use the knob later\n",
    "IMG_SIZE_HR = 128                     # Increased patch size for better GPU utilization\n",
    "\n",
    "# Linux-optimized DataLoader settings\n",
    "NUM_WORKERS = 4 if torch.cuda.is_available() else 0  # Multiple workers for GPU systems\n",
    "PIN_MEMORY = True                     # helps GPU batches\n",
    "PERSISTENT_WORKERS = True if NUM_WORKERS > 0 else False  # Keep workers alive\n",
    "\n",
    "print(f\"Optimal batch size: {BATCH_SIZE}\")\n",
    "print(f\"Workers: {NUM_WORKERS}\")\n",
    "monitor_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21255f17",
   "metadata": {},
   "source": [
    "### Dataset Configuration and Data Loading Pipeline\n",
    "\n",
    "This section sets up the training dataset and data loader infrastructure:\n",
    "- Configures paths for high-resolution training images\n",
    "- Creates on-the-fly paired dataset (HR → LR via bicubic downsampling)\n",
    "- Establishes data loading pipeline with batching and GPU optimization\n",
    "- Handles fallback directories and validates data availability for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".webp\"}\n",
    "\n",
    "class PairedOnTheFlyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    GPU-optimized dataset that loads HR images and creates LR via bicubic downscale.\n",
    "    Returns: {'lr': tensor, 'hr': tensor, 'path': str}\n",
    "    Optimized for CUDA performance with efficient tensor operations.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_hr: Path, fallback: Path | None, scale: int = 4, patch_hr: int | None = None):\n",
    "        self.scale = scale\n",
    "        self.patch_hr = patch_hr\n",
    "        self.root_hr = Path(root_hr)\n",
    "        self.paths = []\n",
    "        \n",
    "        # Scan for images efficiently\n",
    "        if self.root_hr.exists():\n",
    "            self.paths = [p for p in self.root_hr.rglob(\"*\") if p.suffix.lower() in IMG_EXTS]\n",
    "        if not self.paths and fallback and Path(fallback).exists():\n",
    "            self.paths = [p for p in Path(fallback).rglob(\"*\") if p.suffix.lower() in IMG_EXTS]\n",
    "        if not self.paths:\n",
    "            raise FileNotFoundError(\"No images found in data/HR/ or data/samples/. Add a few images to proceed.\")\n",
    "        \n",
    "        print(f\"✓ Found {len(self.paths)} images for training\")\n",
    "\n",
    "    def _center_crop_multiple(self, img: Image.Image, multiple: int) -> Image.Image:\n",
    "        \"\"\"Efficiently crop image to be divisible by scale factor\"\"\"\n",
    "        w, h = img.size\n",
    "        W = (w // multiple) * multiple\n",
    "        H = (h // multiple) * multiple\n",
    "        left = (w - W) // 2\n",
    "        top  = (h - H) // 2\n",
    "        return img.crop((left, top, left + W, top + H))\n",
    "\n",
    "    def _random_crop(self, img: Image.Image, size: int) -> Image.Image:\n",
    "        \"\"\"Random crop for data augmentation during training\"\"\"\n",
    "        w, h = img.size\n",
    "        if w < size or h < size:\n",
    "            # If image is smaller than desired patch, use center crop\n",
    "            return self._center_crop_multiple(img, self.scale)\n",
    "        \n",
    "        # Random crop position\n",
    "        left = random.randint(0, w - size)\n",
    "        top = random.randint(0, h - size)\n",
    "        return img.crop((left, top, left + size, top + size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            path = self.paths[idx]\n",
    "            \n",
    "            # Load and convert to RGB efficiently\n",
    "            with Image.open(path) as img:\n",
    "                hr = img.convert(\"RGB\")\n",
    "            \n",
    "            # Ensure dimensions are divisible by scale\n",
    "            hr = self._center_crop_multiple(hr, self.scale)\n",
    "\n",
    "            # Optional HR patch extraction (random crop for augmentation during training)\n",
    "            if self.patch_hr is not None:\n",
    "                hr = self._random_crop(hr, self.patch_hr)\n",
    "\n",
    "            # Create LR via bicubic downsampling (PIL is optimized for this)\n",
    "            w, h = hr.size\n",
    "            lr_size = (w // self.scale, h // self.scale)\n",
    "            lr = hr.resize(lr_size, Image.Resampling.BICUBIC)\n",
    "\n",
    "            # Convert to tensors efficiently (already normalized to [0,1])\n",
    "            hr_t = TF.to_tensor(hr)\n",
    "            lr_t = TF.to_tensor(lr)\n",
    "            \n",
    "            return {\n",
    "                \"lr\": lr_t,\n",
    "                \"hr\": hr_t, \n",
    "                \"path\": str(path)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            # Return a fallback (first image) to avoid breaking the batch\n",
    "            return self.__getitem__(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "HR_VALID = DATA_ROOT / \"HR\" / \"valid\"\n",
    "HR_TRAIN = DATA_ROOT / \"HR\" / \"train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a9b42a",
   "metadata": {},
   "source": [
    "#### DataLoader + model to GPU\n",
    "We'll reuse `UpscaleGeneratorV1` from earlier in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab1a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset & loader with GPU optimization\n",
    "root_hr = Path(\"../data\") / \"HR\"\n",
    "fallback = Path(\"../data\") / \"samples\"\n",
    "train_ds = PairedOnTheFlyDataset(root_hr, fallback, scale=SCALE, patch_hr=IMG_SIZE_HR)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, \n",
    "                      persistent_workers=PERSISTENT_WORKERS, drop_last=True)\n",
    "\n",
    "# Grab model from previous cell; if needed, re-run its definition.\n",
    "gen = UpscaleGeneratorV1(scale=SCALE, n_feats=64, n_res=4).to(device)\n",
    "print(\"Params (M):\", round(gen.count_params()/1e6, 3))\n",
    "print(\"Model device:\", next(gen.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb9321a",
   "metadata": {},
   "source": [
    "##### Tiny AMP training loop (L1 only, α=0 for now)\n",
    "We’ll do a single-network (generator) optimization against an L1 reconstruction loss. This is not GAN training yet; it’s just to validate the pipeline, GPU, dataloader, and that loss decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf3913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast, GradScaler  # Updated import path for newer PyTorch versions\n",
    "from time import perf_counter\n",
    "\n",
    "# Set model to training mode (enables dropout, batch norm training behavior)\n",
    "gen.train()\n",
    "\n",
    "# Initialize Adam optimizer with specified learning rate and beta parameters\n",
    "# betas=(0.9, 0.99) are slightly different from default (0.9, 0.999) for stability\n",
    "opt = torch.optim.Adam(gen.parameters(), lr=LR, betas=(0.9, 0.99))\n",
    "\n",
    "# Initialize gradient scaler for automatic mixed precision (AMP) training\n",
    "# Only enabled when using CUDA to avoid potential issues on CPU\n",
    "scaler = GradScaler(device=device.type)\n",
    "\n",
    "# Training loop initialization\n",
    "step = 0  # Current training step counter\n",
    "t0 = perf_counter()  # Start time for measuring training duration\n",
    "loss_hist = []  # List to store loss values for analysis\n",
    "\n",
    "# Main training loop over batches\n",
    "for batch in train_dl:\n",
    "    # Stop training after reaching the specified number of steps\n",
    "    if step >= NUM_STEPS:\n",
    "        break\n",
    "    \n",
    "    # Move input tensors to GPU with non-blocking transfer for efficiency\n",
    "    # non_blocking=True allows CPU-GPU transfer to overlap with computation\n",
    "    lr = batch[\"lr\"].to(device, non_blocking=True)  # Low-resolution input images\n",
    "    hr = batch[\"hr\"].to(device, non_blocking=True)  # High-resolution target images\n",
    "\n",
    "    # Zero gradients from previous iteration\n",
    "    # set_to_none=True is more memory efficient than setting to zero\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # Forward pass with automatic mixed precision\n",
    "    # autocast automatically uses float16 for compatible operations to save memory/speed\n",
    "    with autocast(device_type=device.type):\n",
    "        # Generate super-resolved image using current alpha value and no graphics preservation\n",
    "        sr = gen(lr, alpha=ALPHA, preserve_graphics=False)\n",
    "        \n",
    "        # Compute L1 (Mean Absolute Error) loss between generated and target images\n",
    "        # L1 loss encourages sharp reconstruction and is commonly used in super-resolution\n",
    "        loss = F.l1_loss(sr, hr)\n",
    "\n",
    "    # Backward pass with gradient scaling for mixed precision training\n",
    "    # scaler.scale() scales the loss to prevent gradient underflow in float16\n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "    # Optimizer step with gradient unscaling and clipping if needed\n",
    "    # scaler.step() unscales gradients before applying them\n",
    "    scaler.step(opt)\n",
    "    \n",
    "    # Update the gradient scaler's internal state for next iteration\n",
    "    # Adjusts scaling factor based on whether gradients were finite\n",
    "    scaler.update()\n",
    "\n",
    "    # Log training progress\n",
    "    loss_hist.append(loss.item())  # Store loss value for later analysis\n",
    "    \n",
    "    # Print progress every 10 steps\n",
    "    if step % 10 == 0:\n",
    "        print(f\"step {step:03d}  L1: {loss.item():.4f}\")\n",
    "    \n",
    "    step += 1  # Increment step counter\n",
    "\n",
    "# Training completion summary\n",
    "t1 = perf_counter()  # End time\n",
    "print(f\"done {step} steps in {t1 - t0:.2f}s ; last L1={loss_hist[-1]:.4f}\")\n",
    "print(\"mean L1:\", sum(loss_hist)/len(loss_hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e96e5c",
   "metadata": {},
   "source": [
    "Save checkpoint for continuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03285172",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = Path(\"../experiments\") / \"checkpoints\"\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "ckpt_path = ckpt_dir / \"gen_v1_smoke.pt\"\n",
    "\n",
    "torch.save({\n",
    "    \"model\": gen.state_dict(),\n",
    "    \"scale\": SCALE,\n",
    "    \"alpha_hint\": ALPHA,\n",
    "    \"meta\": {\"steps\": step, \"img_size_hr\": IMG_SIZE_HR}\n",
    "}, ckpt_path)\n",
    "\n",
    "print(\"Saved:\", ckpt_path.resolve())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PixelForge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
